{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVTDkq4yAj-R"
      },
      "source": [
        "# Attention and Transformer model\n",
        "In this notebook, we will look at the Transformer architecture using Attention technology (Article: [Attention is All You Need](https://arxiv.org/abs/1706.03762)). Let's implement this architecture with some (many) changes. We also recommend the following materials for a more detailed acquaintance [first](https://www.mihaileric.com/posts/transformers-attention-in-disguise/), [second](https://jalammar.github.io/illustrated-transformer/) and [third](http://nlp.seas.harvard.edu/2018/04/03/attention.html).\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The beauty of the transformer architecture is that it is basically a seq-to-seq model, but not recurrent or autoregressive (i.e., each input and output is processed independently).\n",
        "\n",
        "Transformer has become a state-of-the-art solution for many NLP tasks. In particular, pretrained Bert ([BERT](https://arxiv.org/abs/1810.04805)) is still used as a very good basic embedding for almost any NLP task.\n",
        "\n",
        "In addition to Bert, there are many other powerful pre-trained transformers. They can be found in the [huggingface](https://huggingface.co/transformers/) library. A full list of models can be seen at this [link](https://huggingface.co/transformers/pretrained_models.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to google drive and tensorboard"
      ],
      "metadata": {
        "id": "b9LZYtWixB2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "bSoSSq_Iw_Gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "!rm -rf /content/gdrive/MyDrive/runs\n",
        "%tensorboard --logdir /content/gdrive/MyDrive/runs"
      ],
      "metadata": {
        "id": "hMhLEcTRbKzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aSLCAIzA4vO"
      },
      "source": [
        "## Data processing\n",
        "\n",
        "Let's solve news classification problem [AG_NEWS](https://paperswithcode.com/dataset/ag-news). The task: define the topik (4 topiks exist) of news - “World”, “Sports”, “Business”, “Sci/Tech”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg-z24KZoK2H"
      },
      "outputs": [],
      "source": [
        "LOAD_LIBS = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUanmLxlF1Ct"
      },
      "outputs": [],
      "source": [
        "if LOAD_LIBS:\n",
        "  !pip install transformers -q\n",
        "  !pip install datasets -q\n",
        "  !pip install nltk -q\n",
        "  import nltk\n",
        "  nltk.download('stopwords')\n",
        "  !pip install pymorphy2 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjZUIpEAB_ZU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.ticker as ticker\n",
        "from transformers import get_scheduler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import string\n",
        "import pymorphy2\n",
        "import re\n",
        "\n",
        "# datasets from huggingface\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqXz6tjXFX-E"
      },
      "outputs": [],
      "source": [
        "BASE_FOLDER_PATH = 'gdrive/MyDrive'\n",
        "BERT_MODEL_NAME = 'bert-base-cased'\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "MODEL_NAME = 'model_v0'\n",
        "LR = 0.00005\n",
        "N_HEADS = 1\n",
        "TRAIN_SIZE = 20000\n",
        "TEST_SIZE = 1000\n",
        "\n",
        "MODEL_FOLDER_PATH = os.path.join(BASE_FOLDER_PATH, MODEL_NAME)\n",
        "\n",
        "SCHEDULER_LAMBDA_PARAM = 0.96\n",
        "PAD_IND = 0\n",
        "HIDDEN_DIM = 768\n",
        "\n",
        "ENGLISH_STOP_WORDS = set(stopwords.words('english'))\n",
        "PUNCT_WORD_TOKENIZER = nltk.WordPunctTokenizer() # for preprocess\n",
        "# for broadening your horizons use it for lemmatization\n",
        "MORPH_ANALYZER = pymorphy2.MorphAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(MODEL_FOLDER_PATH, exist_ok=True)"
      ],
      "metadata": {
        "id": "AmSms4nSxZeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THCz8hdiAf3f"
      },
      "outputs": [],
      "source": [
        "# dataset load\n",
        "dataset_dict = load_dataset('ag_news')\n",
        "train_ds = dataset_dict['train'].select(range(TRAIN_SIZE))\n",
        "test_ds = dataset_dict['test'].select(range(TEST_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M-EXW3gxNnXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    nums_filtered_text = re.sub(r'[0-9]+', '', text.lower())\n",
        "    punct_filtered_text = ''.join(\n",
        "        [ch for ch in nums_filtered_text if ch not in string.punctuation]\n",
        "    )\n",
        "    tokens = PUNCT_WORD_TOKENIZER.tokenize(punct_filtered_text)\n",
        "    filtr_stop_words_tokens = [MORPH_ANALYZER.parse(token)[0].normal_form for token in tokens\n",
        "                             if token not in ENGLISH_STOP_WORDS]\n",
        "    norm_tokens = [MORPH_ANALYZER.parse(token)[0].normal_form for token in filtr_stop_words_tokens]\n",
        "\n",
        "    return f\"[CLS] {' '.join(norm_tokens)}\""
      ],
      "metadata": {
        "id": "vzKk8MgUqFbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2tE1g4oJMUd"
      },
      "source": [
        "Let's use Bert tokenizer as tokenizer. And 0 layer of Bert is used as embedding (only layer with embedding without encoder blocks).\n",
        "\n",
        "Such tokenizer and embedding is used to simplify the code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfKhLVyooc1R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ri-SJbSEFwya"
      },
      "outputs": [],
      "source": [
        "def prepare_bert_tokenizer_and_embedder(bert_model_name, device=device):\n",
        "    tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "    bert_embedder = BertModel.from_pretrained(bert_model_name)\n",
        "    bert_embedder.pooler = nn.Identity()\n",
        "    del bert_embedder.encoder.layer[:]\n",
        "\n",
        "    return tokenizer, bert_embedder.to(device)\n",
        "\n",
        "tokenizer, embedder = prepare_bert_tokenizer_and_embedder(bert_model_name=BERT_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GHkHsKTpund"
      },
      "source": [
        "Research text lengths. It could be necessary because we have to synchronize tokens sentence length. But we use Attention mechanism for these reason we can create mask for input tokens to ignore PAD tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHWjBPm2n4WE"
      },
      "outputs": [],
      "source": [
        "lengs = []\n",
        "if FIRST_START:\n",
        "  lengs = [len(tokenizer.tokenize(train_json['text'])) for train_json in tqdm(train_ds)]\n",
        "\n",
        "if lengs:\n",
        "  plt.hist(lengs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyzTk4j4z9xI"
      },
      "source": [
        "Let's write function to create mask by lengths tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9HNr0XnIcbJ"
      },
      "outputs": [],
      "source": [
        "def length_to_mask(length, max_len=None, dtype=None):\n",
        "    \"\"\"length: B.\n",
        "    return B x max_len.\n",
        "    If max_len is None, then max of length will be used.\n",
        "    \"\"\"\n",
        "    assert len(length.shape) == 1, 'Length shape should be 1 dimensional.'\n",
        "    max_len = max_len or length.max().item()\n",
        "    mask = torch.arange(max_len, device=length.device,\n",
        "                        dtype=length.dtype).expand(len(length), max_len) < length.unsqueeze(1)\n",
        "    if dtype is not None:\n",
        "        mask = torch.as_tensor(mask, dtype=dtype, device=length.device)\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwgC43KIKn2D"
      },
      "source": [
        "Next, let's create a cool set of dates. To keep things simple and inexpensive, write the following code so that it immediately returns the text presented in the video in accordance with the embedding sequence.\n",
        "\n",
        "Also, for simplicity, this class will work with a date set object from library data sets (this can easily be corrected under any set of texts in a sequential task)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZ_h0SQVBi5K"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "  def __init__(self, hug_dataset, tokenizer, embedder=None, device=device):\n",
        "    self.hug_dataset = hug_dataset # hugging dataset object\n",
        "    self.tokenizer = tokenizer\n",
        "    self.embedder = embedder\n",
        "    self.device = device\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item_dict = self.hug_dataset[idx]\n",
        "    text = item_dict['text']\n",
        "    normalized_text = preprocess_text(text)\n",
        "    target = item_dict['label']\n",
        "\n",
        "    token_ids = self.text_to_tokens_ids(normalized_text)\n",
        "    tokens_text_len = len(token_ids)\n",
        "    # pad tokens to length\n",
        "    if self.embedder:\n",
        "        return self.embedder(token_ids.unsqueeze(0)), target, tokens_text_len\n",
        "\n",
        "    return token_ids, target\n",
        "\n",
        "  def text_to_tokens_ids(self, text):\n",
        "    tokens = self.tokenizer.tokenize(text)\n",
        "\n",
        "    return torch.tensor(self.tokenizer.convert_tokens_to_ids(tokens)).to(device)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.hug_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpOU8RMqzeNQ"
      },
      "source": [
        "Write the collate_fn (function for DataLoader) to combine batch data to tensors with embeddings tensor, targets_tensor and lengths_tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDonhtnkvdZ5"
      },
      "outputs": [],
      "source": [
        "def collate_batch(batch):\n",
        "  targets_list, embeddings_list, lengths_list = [], [], []\n",
        "\n",
        "  for (_embed_output, _target, _text_len_in_tokens) in batch:\n",
        "    _embed = _embed_output.last_hidden_state\n",
        "    targets_list.append(_target)\n",
        "    embeddings_list.append(_embed[0])\n",
        "    lengths_list.append(_text_len_in_tokens)\n",
        "\n",
        "  targets_tensor = torch.tensor(targets_list, dtype=torch.int64).to(device)\n",
        "  embeddings_tensor = pad_sequence(embeddings_list, batch_first=True, padding_value=PAD_IND).to(device)\n",
        "  lengths_tensor = torch.tensor(lengths_list, dtype=torch.int64).to(device)\n",
        "\n",
        "  return embeddings_tensor.detach(), targets_tensor.detach(), lengths_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKD2I0xvLbgm"
      },
      "source": [
        "Create dataset and dataloader objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86auVS-ABz9N"
      },
      "outputs": [],
      "source": [
        "train_dataset = MyDataset(train_ds, tokenizer=tokenizer, embedder=embedder)\n",
        "test_dataset = MyDataset(test_ds, tokenizer=tokenizer, embedder=embedder)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_batch, drop_last=True)\n",
        "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=3, collate_fn=collate_batch, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LGy08uxwmPv"
      },
      "outputs": [],
      "source": [
        "sentence_samples_to_show_attention = [train_ds[1129], train_ds[4534]]\n",
        "sentence_samples_to_show_attention = [sent['text'] for sent in sentence_samples_to_show_attention]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjELZ_HTL9wd"
      },
      "source": [
        "## Model creation\n",
        "\n",
        "Create the Transform model only with PyTorch library. Let's remember how Attention works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu2aunI-OVsB"
      },
      "source": [
        "Attention can be though of as *queries*, *keys* and *values* - where the query is used with the key to get an attention vector (usually the output of a *softmax* operation and has all values between 0 and 1 which sum to 1) which is then used to get a weighted sum of the values.\n",
        "\n",
        "The Transformer uses *scaled dot-product attention*, where the query and key are combined by taking the dot product between them, then applying the softmax operation and scaling by $d_k$ before finally then multiplying by the value. $d_k$ is the *head dimension*, `head_dim`, which we will shortly explain further.\n",
        "\n",
        "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$\n",
        "\n",
        "This is similar to standard *dot product attention* but is scaled by $d_k$, which the paper states is used to stop the results of the dot products growing large, causing gradients to become too small.\n",
        "\n",
        "However, the scaled dot-product attention isn't simply applied to the queries, keys and values. Instead of doing a single attention application the queries, keys and values have their `hid_dim` split into $h$ *heads* and the scaled dot-product attention is calculated over all heads in parallel. This means instead of paying attention to one concept per attention application, we pay attention to $h$. We then re-combine the heads into their `hid_dim` shape, thus each `hid_dim` is potentially paying attention to $h$ different concepts.\n",
        "\n",
        "$$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^O $$\n",
        "\n",
        "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $$\n",
        "\n",
        "$W^O$ is the linear layer applied at the end of the multi-head attention layer, `fc`. $W^Q, W^K, W^V$ are the linear layers `fc_q`, `fc_k` and `fc_v`.\n",
        "\n",
        "Walking through the module, first we calculate $QW^Q$, $KW^K$ and $VW^V$ with the linear layers, `fc_q`, `fc_k` and `fc_v`, to give us `Q`, `K` and `V`. Next, we split the `hid_dim` of the query, key and value into `n_heads` using `.view` and correctly permute them so they can be multiplied together. We then calculate the `energy` (the un-normalized attention) by multiplying `Q` and `K` together and scaling it by the square root of `head_dim`, which is calulated as `hid_dim // n_heads`. We then mask the energy so we do not pay attention over any elements of the sequeuence we shouldn't, then apply the softmax and dropout. We then apply the attention to the value heads, `V`, before combining the `n_heads` together. Finally, we multiply this $W^O$, represented by `fc_o`.\n",
        "\n",
        "Note that in our implementation the lengths of the keys and values are always the same, thus when matrix multiplying the output of the softmax, `attention`, with `V` we will always have valid dimension sizes for matrix multiplication. This multiplication is carried out using `torch.matmul` which, when both tensors are >2-dimensional, does a batched matrix multiplication over the last two dimensions of each tensor. This will be a **[query len, key len] x [value len, head dim]** batched matrix multiplication over the batch size and each head which provides the **[batch size, n heads, query len, head dim]** result.\n",
        "\n",
        "One thing that looks strange at first is that dropout is applied directly to the attention. This means that our attention vector will most probably not sum to 1 and we may pay full attention to a token but the attention over that token is set to 0 by dropout. This is never explained, or even mentioned, in the paper however is used by the [official implementation](https://github.com/tensorflow/tensor2tensor/) and every Transformer implementation since, [including BERT](https://github.com/google-research/bert/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQiIJ7GeMUE1"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "\n",
        "        assert hid_dim % n_heads == 0\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "\n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "\n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "\n",
        "    def forward(self, query, key, value, mask = None):\n",
        "\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "\n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "\n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "\n",
        "        #energy = [batch size, n heads, query len, key len]\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask[:, None, None, :]\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "\n",
        "        #attention = [batch size, n heads, query len, key len]\n",
        "\n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "\n",
        "        #x = [batch size, n heads, query len, head dim]\n",
        "\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "        #x = [batch size, query len, n heads, head dim]\n",
        "\n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "\n",
        "        #x = [batch size, query len, hid dim]\n",
        "\n",
        "        x = self.fc_o(x)\n",
        "\n",
        "        #x = [batch size, query len, hid dim]\n",
        "\n",
        "        return x, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiVNmziIQ4--"
      },
      "source": [
        "***Question:*** Is the Python class for Encoder self-attention, Decoder self-attention or attention between Encoder and Decoder?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1K3ZC-la4i4"
      },
      "source": [
        "Based on self-attention idea let's construct easy classification model: self-attention on the bert embedings, result of attention for 0 token is used in the linear classification layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiiWnrbGRF7a"
      },
      "outputs": [],
      "source": [
        "class SelfAttentionBasedClassifier(nn.Module):\n",
        "  def __init__(self, hid_dim, cnt_class=4, device=device, n_heads=N_HEADS):\n",
        "    super().__init__()\n",
        "    self.hid_dim = hid_dim\n",
        "    self.device = device\n",
        "    self.cnt_class = cnt_class\n",
        "    self.attn = MultiHeadAttentionLayer(hid_dim=self.hid_dim, n_heads=n_heads, dropout=0, device=self.device)\n",
        "    self.classifier_head = nn.Linear(self.hid_dim, self.cnt_class)\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "  def forward(self, x, mask = None):\n",
        "    x, attention = self.attn(x, x, x, mask=mask)\n",
        "    # let's classifier by 0 token information\n",
        "    # Reason of 0 token\n",
        "    x = x[:, 0, :].squeeze()\n",
        "    x = self.classifier_head(x)\n",
        "\n",
        "    #return self.softmax(x), attention\n",
        "    return x, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjLLHgp3cXdE"
      },
      "outputs": [],
      "source": [
        "model = SelfAttentionBasedClassifier(HIDDEN_DIM).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write functions for display attention matrixes"
      ],
      "metadata": {
        "id": "nU6v19Sgi8X9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attention_fig(sentence, attention, n_heads = 8, n_rows = 4, n_cols = 2):\n",
        "    assert n_rows * n_cols == n_heads\n",
        "\n",
        "    fig = plt.figure(figsize=(12 * n_cols, 12 * n_rows))\n",
        "\n",
        "    for i in range(n_heads):\n",
        "\n",
        "        ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
        "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
        "\n",
        "        cax = ax.matshow(_attention, cmap='bone')\n",
        "\n",
        "        ax.tick_params(labelsize=12)\n",
        "        ticks = [t.lower() for t in sentence]\n",
        "        ax.set_xticklabels(ticks,\n",
        "                           rotation=45)\n",
        "        ax.set_yticklabels(ticks)\n",
        "\n",
        "        ax.xaxis.set_major_formatter(ticker.FixedFormatter(ticks))\n",
        "        ax.yaxis.set_major_formatter(ticker.FixedFormatter(ticks))\n",
        "        #ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        #ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    return fig"
      ],
      "metadata": {
        "id": "5_sX3ylqgW5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_attention_examples(\n",
        "    tb_writer,\n",
        "    texts=sentence_samples_to_show_attention,\n",
        "    tokenizer=tokenizer,\n",
        "    embedder=embedder,\n",
        "    n_heads=N_HEADS,\n",
        "    n_rows=1,\n",
        "    n_cols=1):\n",
        "  cur_dataset = MyDataset([{'text': text, 'label': -1} for text in texts], tokenizer, embedder)\n",
        "  cur_dataloader = DataLoader(cur_dataset, batch_size=1, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for text, data in zip(texts, cur_dataloader):\n",
        "        input_data, _, lengths = data\n",
        "        mask = length_to_mask(lengths)\n",
        "\n",
        "        input_data = input_data.to(device)\n",
        "        outputs, attention = model(input_data, mask=mask)\n",
        "\n",
        "        att_fig = get_attention_fig(tokenizer.tokenize(text), attention, n_heads=n_heads, n_rows=n_rows, n_cols=n_cols)\n",
        "        tb_writer.add_figure(f'Text attention: {text}', att_fig)"
      ],
      "metadata": {
        "id": "ykrC_NfuhN8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BLSjMH6ehHvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write functions for training and validation process."
      ],
      "metadata": {
        "id": "M9p9RzxIwc9B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zShqJMucs1O"
      },
      "outputs": [],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "lambda_scheduler = lambda x: SCHEDULER_LAMBDA_PARAM ** x\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_scheduler)\n",
        "\n",
        "writer = SummaryWriter(os.path.join(BASE_FOLDER_PATH, 'runs/{}'.format(MODEL_NAME)))\n",
        "\n",
        "def train_one_epoch(epoch_index, model, training_loader, scheduler, optimizer, loss_fn, tb_writer):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "    train_loss = 0.\n",
        "\n",
        "    i = 0\n",
        "    cnt_right_answers = 0\n",
        "    cnt_answers = 0\n",
        "\n",
        "    all_answers = []\n",
        "    answers_probs = []\n",
        "    all_labels = []\n",
        "    for data in tqdm(training_loader):\n",
        "        input_data, labels, lengths = data\n",
        "        mask = length_to_mask(lengths)\n",
        "\n",
        "        if epoch_index != -1:\n",
        "            optimizer.zero_grad()\n",
        "        input_data = input_data.to(device)\n",
        "        labels = labels.to(device).long()\n",
        "        outputs, attention = model(input_data, mask=mask)\n",
        "\n",
        "        answers = outputs.argmax(axis=-1)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        if epoch_index != -1:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        train_loss += loss.item()\n",
        "        if i % 10 == 9 and epoch_index != -1:\n",
        "            last_loss = running_loss / 10 # loss per batch\n",
        "            tb_x = epoch_index * len(training_loader) + i + 1\n",
        "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "            running_loss = 0.\n",
        "\n",
        "            if i % 10 == 0 and scheduler is not None:\n",
        "                tb_writer.add_scalar('Scheduler LR', optimizer.param_groups[0][\"lr\"], tb_x)\n",
        "                scheduler.step()\n",
        "            if i % 100 == 99:\n",
        "                display_attention_examples(tb_writer)\n",
        "\n",
        "        cnt_answers += labels.shape[0]\n",
        "        cnt_right_answers += (answers == labels).sum().item()\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    train_loss = train_loss / len(training_loader)\n",
        "    tb_writer.add_scalar('Accuracy/train', cnt_right_answers / cnt_answers, epoch_index + 1)\n",
        "\n",
        "    return train_loss\n",
        "\n",
        "\n",
        "def validation(epoch_number, val_dataloader):\n",
        "    val_loss = 0.0\n",
        "\n",
        "    all_answers = []\n",
        "    answers_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    cnt_answers = 0\n",
        "    cnt_right_answers = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, vdata in tqdm(enumerate(val_dataloader)):\n",
        "            input_data, labels, lengths = vdata\n",
        "            mask = length_to_mask(lengths)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            input_data = input_data.to(device)\n",
        "            labels = labels.to(device).long()\n",
        "            outputs, attention = model(input_data, mask=mask)\n",
        "\n",
        "            vloss = loss_fn(outputs, labels)\n",
        "            val_loss += vloss.item()\n",
        "            answers = outputs.argmax(axis=-1)\n",
        "\n",
        "            cnt_answers += labels.shape[0]\n",
        "            cnt_right_answers += (answers == labels).sum().item()\n",
        "\n",
        "            del input_data, labels, lengths\n",
        "\n",
        "    val_loss = val_loss / len(val_dataloader)\n",
        "    print(\"VAL LOSS =\", val_loss)\n",
        "    return val_loss, cnt_right_answers / cnt_answers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start train loop"
      ],
      "metadata": {
        "id": "fyKKz0egwsm1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1IhUYAhdQn6"
      },
      "outputs": [],
      "source": [
        "best_vloss = 1_000_000.\n",
        "\n",
        "for epoch_number in tqdm(range(EPOCHS)):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    model.train()\n",
        "    train_loss = train_one_epoch(\n",
        "        epoch_index=epoch_number,\n",
        "        model=model,\n",
        "        training_loader=train_dataloader,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        loss_fn=loss_fn,\n",
        "        tb_writer=writer)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_acc = validation(epoch_number, test_dataloader)\n",
        "\n",
        "    writer.add_scalar('Loss/valid', val_loss, epoch_number + 1)\n",
        "    writer.add_scalar('Accuracy/valid', val_acc, epoch_number + 1)\n",
        "    if val_loss < best_vloss:\n",
        "        best_vloss = val_loss\n",
        "        model_path = os.path.join(MODEL_FOLDER_PATH, 'model_{}_{}'.format(epoch_number + 1, timestamp))\n",
        "        torch.save(model.state_dict(), model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5Kq_wsHd-RN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train the BERT model for this task\n",
        "\n"
      ],
      "metadata": {
        "id": "Y1Us-Ooqtoq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is based on the [example](https://huggingface.co/docs/transformers/tasks/sequence_classification) from huggingface."
      ],
      "metadata": {
        "id": "c5LKLo44Fi2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "7eSafa3ytxHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model, tokenizer, optimizer, scheduler  # delete our previous objects"
      ],
      "metadata": {
        "id": "YF7EWTjs_-Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download pretrained models\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4).to(device)\n",
        "for w in model.distilbert.parameters():\n",
        "    w._trainable= False\n",
        "for w in model.classifier.parameters():\n",
        "    w._trainable = True"
      ],
      "metadata": {
        "id": "CAAMavVnuIbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare dataset"
      ],
      "metadata": {
        "id": "elcrM_sqANgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dict = load_dataset('ag_news')\n",
        "train_ds = dataset_dict['train']\n",
        "test_ds = dataset_dict['test']"
      ],
      "metadata": {
        "id": "namDj2m3ubQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples, tokenizer=tokenizer):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        return_token_type_ids=\"token_type_ids\" in tokenizer.model_input_names)\n",
        "\n",
        "def ds_object_process(ds_obj):\n",
        "  return ds_obj.map(\n",
        "        tokenize_function, batched=True\n",
        "      ).remove_columns([\"text\"]).rename_column(\"label\", \"labels\")\n",
        "\n",
        "# Take small train to speed up train\n",
        "tokenized_train_dataset = ds_object_process(train_ds.select(range(500)))\n",
        "tokenized_train_dataset.set_format(\"torch\")\n",
        "tokenized_test_dataset = ds_object_process(test_ds.select(range(TEST_SIZE)))\n",
        "tokenized_test_dataset.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "-3PGTAIouTT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_nG-Pp3KBgSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create dataloaders, optimizer and scheduler"
      ],
      "metadata": {
        "id": "b0sMq6RMFW_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(tokenized_train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
        "test_dataloader = DataLoader(tokenized_test_dataset, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "4PWK8whjvA0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
        "num_training_steps = EPOCHS * len(train_dataloader)\n",
        "scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")"
      ],
      "metadata": {
        "id": "NUeMihZvvIsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Qz8D5XA-ubf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run train loop"
      ],
      "metadata": {
        "id": "swknlY5lFb9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    train_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "    train_loss /= len(train_dataloader)\n",
        "    print(f'EPOCH {epoch}: train_loss={train_loss}')"
      ],
      "metadata": {
        "id": "gU_RYa1svQBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run validation"
      ],
      "metadata": {
        "id": "vg4N99geFeh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "model.eval()\n",
        "for batch in tqdm(test_dataloader):\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "metric.compute()"
      ],
      "metadata": {
        "id": "5Leh1ck6wHLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jQk--uyOw0WE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BFRxn2UhB3Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tCc1pUZ7B8ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7MMqXnkzB-uC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}